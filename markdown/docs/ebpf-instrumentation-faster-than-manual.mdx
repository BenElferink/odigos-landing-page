---
pubDate: 'Oct 16 2023'
title: 'Surprise! eBPF-Based Auto-Instrumentation Faster Than Manual Instrumentation'
image: '/ebpf_vs_manual.png'
category: 'Performance'
description: 'This blog will share the results of our performance tests, which show that eBPF-based automatic instrumentation is up to 3.5X faster than manually instrumenting code.'
tags: [ebpf, performance, opentelemetry, instrumentation]
authorImage: '/eden.jpg'
author: Eden Federman
metadata: This blog will share the results of our performance tests, which show that eBPF-based automatic instrumentation is up to 3.5X faster than manually instrumenting code.
---

# Introduction
The primary feedback we’ve received from engineers who are adopting distributed tracing is that adding tracing to their code requires too much effort. 

To address this issue, we created an [eBPF-based auto-instrumentation for Go applications](https://github.com/open-telemetry/opentelemetry-go-instrumentation), which we donated to the OpenTelemetry community. We then integrated it into our open-source observability control plane, [Odigos](https://github.com/keyval-dev/odigos), which allows you to generate distributed tracing instantly.

The second most popular issue we hear engineers complain about is the negative performance impact adding distributed tracing has on their applications. It turns out that instrumenting your code manually by installing SDKs and calling APIs to generate distributed tracing, still has a (significant) negative performance impact.

We have been working hard on solving this problem. Today, we're excited to share the results of our performance tests, which show that **eBPF-based automatic instrumentation is up to 3.5X faster than manually instrumenting code.**

## How we tested

Benchmarking latency is not a trivial task. Latency can be measured in many different ways, each with its own advantages and disadvantages. Our testing is inspired by this excellent talk by Gil Tene called [How NOT to Measure Latency](https://www.youtube.com/watch?v=lJ8ydIuPFeU). We are using a [High Dynamic Range Histogram](http://hdrhistogram.org/) to visualize the results and [wrk2](https://github.com/giltene/wrk2) to generate load.

In order to reduce noise as much as possible, we are running each test on a dedicated AWS bare metal instance (c5n.metal) with Intel Xeon Platinum 8000 series (Skylake-SP) processor.

For the target application, we are using a simple Go HTTP server written in Go 1.21 that returns a simple JSON response.

## Test results

Each test is run for 5 minutes and generates 10,000 requests per second.
First, we ran the test without any instrumentation. Then we ran the test with manual instrumentation using [OpenTelemetry Go SDK](https://github.com/open-telemetry/opentelemetry-go) and finally, we ran the test with [eBPF-based automatic instrumentation](https://github.com/open-telemetry/opentelemetry-go-instrumentation).

![Test Results](/images/blog/ebpf-vs-manual/results.png)


At the lower percentiles (up to the 99.9th percentile), the overhead of not having instrumentation, manual instrumentation, and eBPF-based automatic instrumentation is similar

However, at the higher percentiles (99.9th percentile and above), manual instrumentation has a significantly higher overhead than eBPF-based automatic instrumentation, which is up to 3.5x faster.

If you're wondering whether the top of the spectrum matters, the answer is yes, especially in distributed environments. The following table shows how many clients will experience the 99th percentile latency according to the number of different services involved in the request (taken from Gil's talk)

![p99 is a lie](/images/blog/ebpf-vs-manual/p99_is_a_lie.png)

## **The performance impact of generating distributed traces**

Let's see what happens inside our application when we generate distributed traces, either manually via SDKs or automatically via something like a Java agent or monkey patching:

1. **Recording data** - Spans objects that contain the relevant data are being created
2. **Maintaining a queue of data** - Spans are being batched in a queue before being sent to the external system
3. **Delivering data to the external system** - The application sends the data in the queue to the external system by making network calls, serializing the data, and sending it over the network.

Another consideration to keep in mind is that the application runtime must now manage more objects, which can negatively impact heap size and GC performance, especially in languages with stop-the-world GC like Java. All this can lead to longer GC pauses and decreased performance. We'll dive deeper and explore this topic in a separate blog post. Stay tuned.

Unlike metrics and logs, distributed tracing is a stateful signal. Logs are typically written to a file, and metrics are typically pulled from an HTTP endpoint by the monitoring system (for example `/metrics` endpoint when exposing Prometheus metrics). Distributed tracing is different. It requires the application to actively send batched data to an external system.

## Separation between recording and processing

When using eBPF to generate distributed traces, we are separating the recording of the data from the processing of the data. The recording of the data is done by the eBPF program and the processing and delivery of the data is done by a different process. This means that the application runtime does no additional work, creates no additional objects, and makes no additional network calls. The only additional overhead (compared to not having any instrumentation) is the overhead of invoking eBPF programs (context switching from user space to kernel space).

## More updates coming soon

eBPF-based automatic instrumentation is a game-changer, enabling us to generate distributed traces **without code changes** or **performance impact**. We're just getting started and will bring this to more programming languages soon. Stay tuned!

## Try it out

The easiest way to try eBPF-based distributed tracing today is to use our [open-source project, Odigos](https://github.com/keyval-dev/odigos). Please support us by starring ⭐ the project on [GitHub](https://github.com/keyval-dev/odigos).